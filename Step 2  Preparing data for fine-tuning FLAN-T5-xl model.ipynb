{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1d04c28-2f11-4669-8d74-2fbba468afda",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset , Dataset, concatenate_datasets \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea320aec-fdd1-422e-98f1-e97e8eb4da99",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Create Pandas dataset from Huggingface Dataset at \"b-mc2/sql-create-context\" at https://huggingface.co/datasets/b-mc2/sql-create-context\n",
    "The creator of the dataset has done some excellent preprocessing after combining data from the open tex to sql datasets WikiSQL and Spider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cf56f09-af44-4d70-95bd-ade4ed6f8d5b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ds = load_dataset(\"b-mc2/sql-create-context\")\n",
    "dspd = pd.DataFrame(ds)\n",
    "display(dspd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "02aac8da-1b87-4108-ba21-0d022736600f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Normalize the json objects in each record to a tabular form with separate columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "926cf3fd-bf2e-41c6-9c92-6fcdac2c14c4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json \n",
    "dsdf_ = pd.json_normalize(dspd['train'])\n",
    "display(dsdf_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d351270c-7e2d-42b2-840e-043961e8a4eb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define a function to split and process each value in the 'context' column\n",
    "def split_clean_ctas(statement):\n",
    "    statements = statement.split(\";\")\n",
    "    statements = [s.replace(\"CREATE TABLE\", \"\").replace(\"VARCHAR\", \"STRING\").strip() for s in statements if s.strip()]\n",
    "    return statements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95c4911d-a4b7-41d7-9a6c-e6e6575acaa7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Apply this to the context column of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8d47cdb-dca8-4734-a6cd-fa85853c5bca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dsdf_['context'] = dsdf_['context'].apply(lambda x: split_clean_ctas(x)).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec8accc7-5ccd-42c9-8050-71db8f340030",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dsdf_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2c8d32a-1b17-4a7e-b17e-147b60eb6d84",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dsdf_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c1e8afa4-7ae1-4134-988a-be4ac1ba9ca5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The goal of the next few code cells is to create a 'context' for the model that consists of more than just the relevant table for the given question. This is to ensure that the model learns to select the schema of the most relevant table if provided few relevant ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "750de484-efab-448b-aafd-5265b171793a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "unique_items = dsdf_['context'].explode().unique().tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bb2daf3-3afb-421b-be60-47566a4edaeb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dsdf_['count'] = dsdf_['context'].apply(lambda x: len(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d739495-7e1b-4916-ad91-acabcb21c222",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Here, for 30% of records where the query is based off of a single table, we add another different table schema to the context. For another 20% we add another 2 table schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6fa05a0-8868-4448-b1c5-ab94076cdeaa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the number of entries for each condition\n",
    "total_count1_entries = int(0.3 * dsdf_[dsdf_['count'] == 1].shape[0])\n",
    "total_count2_entries = int(0.2 * dsdf_[dsdf_['count'] == 1].shape[0])\n",
    "\n",
    "# Get the indices of entries with count==1\n",
    "count1_indices = dsdf_[dsdf_['count'] == 1].index\n",
    "\n",
    "# Append random items to the 'tables' column based on the conditions\n",
    "for i in count1_indices[:total_count1_entries]:\n",
    "    existing_tables = dsdf_.loc[i, 'context']\n",
    "    random_item = random.choice([item for item in unique_items if item not in existing_tables])\n",
    "    dsdf_.at[i, 'context'].append(random_item)\n",
    "\n",
    "for i in count1_indices[total_count1_entries:total_count1_entries+total_count2_entries]:\n",
    "    existing_tables = dsdf_.loc[i, 'context']\n",
    "    random_items = random.sample([item for item in unique_items if item not in existing_tables], 2)\n",
    "    dsdf_.at[i, 'context'].extend(random_items)\n",
    "\n",
    "# Drop the 'count' column\n",
    "dsdf_.drop('count', axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfa1fc4a-16ee-4e7c-8fcb-f4f88510bc71",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dsdf_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6285a80-bf5c-4e7f-a4d7-897eba5a2d5b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dsdf_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dad2e816-aed8-456f-9b56-b6e5871a3d5b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The next step is to shuffle the table schema in the context, such that the model does not learn to 'cheat' by picking the very first table schema in the list of schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72856f61-066a-49cc-a5a7-d1edb410974e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#To shuffle the table names\n",
    "dsdf_['context'] = dsdf_['context'].apply(lambda x: random.sample(x, len(x)))\n",
    "display(dsdf_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c79cf09-eab9-45ab-8bf6-6c6dc04aa7c3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "In the next few cells, we convert the context list to a string and combine the question and context in a specific format, such that it can be treated as a sequence to sequence pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbc5711b-c2c9-4181-bdef-0fb6301abf72",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dsdf_['context'] = dsdf_['context'].apply(lambda x: ', '.join(map(str, x)))\n",
    "display(dsdf_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d5d9493-f515-4458-91ce-b78de82b277c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dsdf_['question_formatted'] = dsdf_['question'] + ', schema: ' + dsdf_['context']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8458ce83-845f-4fab-98cc-f3997e922db7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dsdf_ = dsdf_.rename(columns={'answer': 'query'})\n",
    "dsdf_final = dsdf_[['question_formatted', 'query']]\n",
    "spdf_final = spark.createDataFrame(dsdf_final)\n",
    "display(spdf_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9301bd80-3962-4cb1-b3a0-2d95b75c23e5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Write this out to a table so that it can be easily retrieved for finetuning our model later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "889441c1-712b-46f7-8f20-884883bf99f3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spdf_final.write.saveAsTable('hf_sql_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35cf19ea-51b3-41e5-ad0b-bbe466b5c01e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM\n",
    "hive_metastore.default.hf_sql_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0de7f71a-2efb-4363-91ce-c7e269710780",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 753809234860200,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "Step 0: Preparing data for fine-tuning FLAN-T5-xl model",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
