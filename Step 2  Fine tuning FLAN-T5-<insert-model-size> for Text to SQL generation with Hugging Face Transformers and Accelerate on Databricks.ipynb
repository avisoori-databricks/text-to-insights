{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d571167-c821-4921-9e76-d23877a4fccc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Install all the necessary packages. Make sure to use the Databricks Machine Learning Runtime. Please note that for efficient int 8 training we are using bitandbytes which requires 8-bit tensor core-supported hardware, which are Turing and Ampere GPUs (RTX 20s, RTX 30s, A40-A100, T4+). Choose the appropriate GPU instances to run this. We are installing PEFT for effificient training with LoRa from the source and specific versions of the libraries that will work well together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3723946-e5b4-45bb-b9ce-472b66045b53",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# install Hugging Face Libraries\n",
    "%pip install git+https://github.com/huggingface/peft.git\n",
    "%pip install \"transformers==4.27.2\" \"datasets==2.9.0\" \"accelerate==0.17.1\" \"evaluate==0.4.0\" \"bitsandbytes==0.37.1\" --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bac807a-8fac-4dfb-93f9-89890537e7e3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "import numpy as np\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, default_data_collator, get_linear_schedule_with_warmup\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "13507194-397c-48a4-ac92-856d38a98fa9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Write a basic configuration for accelerate and set the mixed precision mode. Accelerate allows for efficient distributed training without any major changed to the training loop. This speeds up training quite a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9b7676f-bd1a-4e7b-a17e-2b99993b2546",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "python -c \"from accelerate.utils import write_basic_config; write_basic_config(mixed_precision='bf16')\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "686bb81f-770b-4ffa-987a-83fb53b0a68e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The configuration looks like this. Please note the \t- num_processes parameter. This gives you the number of GPUs you have available to train on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cc3dbc0-edc2-4d54-af25-fa174350ca6a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "accelerate env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "868ff251-dbb1-40a7-a81b-b4fe9dbb45e7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The dataset we will be using for finetuning looks like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9663baad-dcf4-4398-b6ea-777d83e792c4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM\n",
    "hive_metastore.default.hf_sql_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "87df4f4f-27b5-4e4c-80d9-9226516f6ae9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Read it into a dataframe and shuffle it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a32aa92d-c39b-4c72-b993-d94bca64237f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "seqtext = spark.sql(\"SELECT * FROM hive_metastore.default.hf_sql_dataset\")\n",
    "seqtext_pd = seqtext.toPandas().sample(frac =1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "950a041c-d0ac-45d7-b2dd-11c0ab3676f3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "seqtext_pd.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4648c395-99d6-4e8e-9f8f-d57fb8600254",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Define a preprocessing function. Notice how we have nestled the sequence to be translated to SQL between \"Translate English to SQL: \" and \". </s>\". Empirically, this seems to have given the best finetuning results as FLAN-T5 was trained with such a string formatting for translating tasks according to the original publication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bdbf744-3f8c-4ddf-9877-618fc214aa8e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    " def preprocess_function(sample,padding=\"max_length\"):\n",
    "  # add prefix to the input for t5\n",
    "  inputs = [\"Translate English to SQL: \" + item + \". </s>\" for item in sample[\"question_formatted\"]]\n",
    "\n",
    "  # tokenize inputs\n",
    "  model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True)\n",
    "\n",
    "  # Tokenize targets with the `text_target` keyword argument\n",
    "  labels = tokenizer(text_target=sample[\"query\"], max_length=max_target_length, padding=padding, truncation=True)\n",
    "\n",
    "  # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n",
    "  # padding in the loss.\n",
    "  if padding == \"max_length\":\n",
    "      labels[\"input_ids\"] = [\n",
    "          [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "     ]\n",
    "\n",
    "  model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "  return model_inputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "798d7038-a757-4f93-a2b1-789387256c88",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Depending on the resources you have available, choose the model size. The trained model in the Hub we have available is fine-tuned FLAN-T5-XL. But for the sake of this example we go with flan-t5-base. Download the tokenizer and model. Cache the model in dbfs so that there are no issues when the model is fetched from the local environment during the training loop encapsulated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fda7e9b0-3b48-4a8a-8f0b-e2f70ee57c20",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Safe option for initially trying out. But with smaller model size (i.e. parameters), the performance will suffer for this example\n",
    "model_id = \"google/flan-t5-base\"\n",
    "#Uncomment if you have resources\n",
    "#model_id = \"google/flan-t5-xl\"  \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d57ce2d-43fe-4cc6-b634-2b1bc7e3b2cf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cache =  \"/dbfs/FileStore/shared_uploads/avinash.sooriyarachchi@databricks.com/text_to_sql/flanbase_accelerate/cache\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d94e51a1-26e4-413d-870b-fa6e737dd80a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id,  torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46093069-c04c-47de-954f-f7e8b1b9f9af",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(cache, from_pt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "354adf3f-9de3-4b46-bd05-2ec538ceb787",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Just testing if the model was cached properly\n",
    "#model = AutoModelForSeq2SeqLM.from_pretrained(cache,  torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ee09ed0-a136-4dff-b0bb-73df0ea48341",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The maximum total input sequence length after tokenization. Sequences longer than this will be truncated, sequences shorter will be padded.\n",
    "Do the same for the target strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f937b92-3cb7-499d-8f3f-16e421c05a64",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset , Dataset, concatenate_datasets \n",
    "\n",
    "dataset = Dataset.from_pandas(seqtext_pd).train_test_split(test_size=0.02)\n",
    "dataset[\"train\"] = dataset[\"train\"].remove_columns([\"__index_level_0__\"])\n",
    "dataset[\"test\"] = dataset[\"test\"].remove_columns([\"__index_level_0__\"])  \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "tokenized_inputs = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]]).map(lambda x: tokenizer(x[\"question_formatted\"], truncation=True), batched=True, remove_columns=[\"question_formatted\", \"query\"])\n",
    "input_lenghts = [len(x) for x in tokenized_inputs[\"input_ids\"]]\n",
    "# take 99th percentile of max length for better utilization\n",
    "max_source_length = int(np.percentile(input_lenghts, 99))\n",
    "#print(f\"Max source length: {max_source_length}\")\n",
    "\n",
    "# The maximum total sequence length for target text after tokenization.\n",
    "# Sequences longer than this will be truncated, sequences shorter will be padded.\"\n",
    "tokenized_targets = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]]).map(lambda x: tokenizer(x[\"query\"], truncation=True), batched=True, remove_columns=[\"question_formatted\", \"query\"])\n",
    "target_lenghts = [len(x) for x in tokenized_targets[\"input_ids\"]]\n",
    "# take 99th percentile of max length for better utilization\n",
    "max_target_length = int(np.percentile(target_lenghts, 99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d8e919b-148f-4e55-8580-4616a9d41aed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "max_source_length, max_target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96f348ab-16b0-4f8d-b457-fb337aab8a9d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def training_function():#model):\n",
    "  import evaluate\n",
    "  import os\n",
    "  import torch\n",
    "  from accelerate import Accelerator\n",
    "  import numpy as np\n",
    "  from torch.utils.data import DataLoader\n",
    "  from tqdm import tqdm\n",
    "\n",
    "  from peft import LoraConfig, TaskType, get_peft_model, get_peft_config, get_peft_model_state_dict, prepare_model_for_int8_training \n",
    "  from peft.utils.other import fsdp_auto_wrap_policy\n",
    "  import sentencepiece\n",
    "  from transformers import DataCollatorForSeq2Seq\n",
    "  accelerator = Accelerator()\n",
    "\n",
    "\n",
    "  #print(f\"Max target length: {max_target_length}\")\n",
    "\n",
    "  processed_datasets = dataset.map(\n",
    "            preprocess_function,\n",
    "            batched=True,\n",
    "            num_proc=1,\n",
    "            remove_columns=[\"question_formatted\", \"query\"],\n",
    "            load_from_cache_file=False,\n",
    "            desc=\"Running tokenizer on dataset\",\n",
    "        )\n",
    "\n",
    "  train_dataset = processed_datasets[\"train\"]\n",
    "  eval_dataset = processed_datasets[\"test\"]\n",
    "\n",
    "  # To have only one message (and not 8) per logs of Transformers or Datasets, we set the logging verbosity\n",
    "  # to INFO for the main process only.\n",
    "  lora_config = LoraConfig(\n",
    "            r=16,\n",
    "            lora_alpha=32,\n",
    "            target_modules=[\"q\", \"v\"],\n",
    "            lora_dropout=0.05,\n",
    "            bias=\"none\",\n",
    "            task_type=TaskType.SEQ_2_SEQ_LM\n",
    "            )\n",
    "  model_id = \"google/flan-t5-base\"\n",
    "  #model = AutoModelForSeq2SeqLM.from_pretrained(model_id,  torch_dtype=torch.bfloat16)\n",
    "  cache =  '<cache-path>'\n",
    "  model = AutoModelForSeq2SeqLM.from_pretrained(cache,  torch_dtype=torch.bfloat16)\n",
    "  model = prepare_model_for_int8_training(model)\n",
    "  model = get_peft_model(model, lora_config)\n",
    "\n",
    "  # we want to ignore tokenizer pad token in the loss\n",
    "  label_pad_token_id = -100\n",
    "  # Data collator\n",
    "  data_collator = DataCollatorForSeq2Seq(\n",
    "      tokenizer,\n",
    "      model=model,\n",
    "      label_pad_token_id=label_pad_token_id,\n",
    "      pad_to_multiple_of=8\n",
    "  )\n",
    "\n",
    "  batch_size = 8\n",
    "  train_dataloader = DataLoader(train_dataset, shuffle=True, collate_fn=data_collator, batch_size=batch_size, pin_memory=True)\n",
    "\n",
    "  eval_dataloader = DataLoader(eval_dataset, collate_fn=data_collator, batch_size=batch_size, pin_memory=True)\n",
    "    \n",
    "\n",
    "  #A lower training rate to begin with is generally better\n",
    "  lr = 1e-4\n",
    "  #2-5 Epochs may be sufficient initially\n",
    "  num_epochs = 4\n",
    "  \n",
    "\n",
    "  optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "  lr_scheduler = get_linear_schedule_with_warmup(\n",
    "      optimizer=optimizer,\n",
    "      num_warmup_steps=0,\n",
    "      num_training_steps=(len(train_dataloader) * num_epochs),\n",
    "  )\n",
    "\n",
    "  \n",
    "  model, train_dataloader, eval_dataloader, optimizer, lr_scheduler = accelerator.prepare(\n",
    "      model, train_dataloader, eval_dataloader, optimizer, lr_scheduler\n",
    "  )\n",
    "  accelerator.print(model)\n",
    "  # Instantiate a progress bar to keep track of training. Note that we only enable it on the main\n",
    "  progress_bar = tqdm(range(num_epochs * len(train_dataloader)), disable=not accelerator.is_main_process)\n",
    "  # Now we train the model\n",
    "  for epoch in range(num_epochs):\n",
    "      model.train()\n",
    "      total_loss = 0\n",
    "      for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "          outputs = model(**batch)\n",
    "          loss = outputs.loss\n",
    "          total_loss += loss.detach().float()\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "          lr_scheduler.step()\n",
    "          optimizer.zero_grad()\n",
    "\n",
    "      model.eval()\n",
    "      eval_loss = 0\n",
    "      #eval_preds = []\n",
    "      #eval_refs = []\n",
    "      for step, batch in enumerate(tqdm(eval_dataloader)):\n",
    "          with torch.no_grad():\n",
    "              outputs = model(**batch)\n",
    "          loss = outputs.loss\n",
    "          eval_loss += loss.detach().float()\n",
    "          #preds = accelerator.gather_for_metrics(torch.argmax(outputs.logits, -1)).detach().cpu().numpy()\n",
    "          #eval_preds.extend(tokenizer.batch_decode(preds, skip_special_tokens=True))\n",
    "          #refs = accelerator.gather_for_metrics(batch[\"labels\"].detach().cpu().numpy())\n",
    "          #items=[[item] for item in tokenizer.batch_decode(refs, skip_special_tokens=True)]\n",
    "          #eval_refs.extend(items)\n",
    "          \n",
    "      eval_epoch_loss = eval_loss / len(eval_dataloader)\n",
    "      eval_ppl = torch.exp(eval_epoch_loss)\n",
    "      train_epoch_loss = total_loss / len(train_dataloader)\n",
    "      train_ppl = torch.exp(train_epoch_loss)\n",
    "      accelerator.print(f\"{epoch=}: {train_ppl=} {train_epoch_loss=} {eval_ppl=} {eval_epoch_loss=}\")\n",
    "      accelerator.wait_for_everyone()\n",
    "      #Save the model for persistent storage such that it can be retrieved later, tested and deployed\n",
    "      base_dir = '<base-dir>'\n",
    "      model_save = '<model-name>'\n",
    "      save_path = base_dir+model_save\n",
    "      model.module.save_pretrained(save_path, state_dict=accelerator.get_state_dict(model))\n",
    "      accelerator.wait_for_everyone()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad8f2a62-6b4c-41d1-a0b5-dd6310f8cc81",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from accelerate import notebook_launcher\n",
    "\n",
    "notebook_launcher(training_function,num_processes=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5730f8fd-375b-4d78-b437-d4c9bb54db9b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Inference testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "707cd365-6de0-4d15-8b4a-535edf7266b8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Redefining things here if there's a need to try out inference separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d57beb07-8ec1-4a99-b7e6-2658a4ccd2b2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "base_dir = '<base-dir>'\n",
    "model_save = '<model-name>'\n",
    "peft_model_id = base_dir+model_save\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(cache)\n",
    "model = PeftModel.from_pretrained(model, peft_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3fb7dac2-cdde-4854-8ea3-e75f530cb807",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "input_text = \"Translate English to SQL: How many people living in the Manhattan make more than 100000 a year?, schema: wages (ID INTEGER, city STRING, state STRING, salary INTEGER)\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "outputs = model.generate(input_ids=inputs[\"input_ids\"], max_new_tokens=100)\n",
    "print( tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc3fe098-00c5-480c-b1f5-19dd63947a22",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Log with MLFlow, wrap in a custom python function, with an inference example and deploy with Databricks model serving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15d67881-ecad-4c8d-b3fc-6183b4c4d0db",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Create an inference example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2cbc6448-2f7a-49ae-b269-15e3fa2481d1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "payload_pd = pd.DataFrame([[input_text]],columns=['text'])\n",
    "input_example = payload_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58749893-1499-4fc6-8a3a-085565caadd7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "It's interesting to note that during the finetuning process, we are only finetuning a few parameters and saving a LoRa adapter, that could be affixed to the layers of the base model and used for inference/ deployed. This adapter has to be wrapped into the pyfunc alongside the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ac6f76f-6082-4742-84d9-86ad6c26faa8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tokenizer_path = '/tmp/tokenizer/'\n",
    "pretrained_model_path = cache\n",
    "lora_adatper_path = base_dir+model_save\n",
    "tokenizer.save_pretrained(tokenizer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b35fa79-36d3-4a6c-98e6-43c8bf0d6bc2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow.pyfunc\n",
    "\n",
    "class TextToSQL(mlflow.pyfunc.PythonModel):\n",
    "  def load_context(self, context):\n",
    "    from peft import PeftModel, PeftConfig\n",
    "    from peft import get_peft_config, get_peft_model, get_peft_model_state_dict, LoraConfig, TaskType\n",
    "    from peft import LoraConfig, get_peft_model, prepare_model_for_int8_training, TaskType\n",
    "    from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "    self.config = PeftConfig.from_pretrained(context.artifacts[\"lora_adatper_path\"])\n",
    "    self.tokenizer = AutoTokenizer.from_pretrained(context.artifacts['tokenizer_path'])\n",
    "    self.model_pretrained = AutoModelForSeq2SeqLM.from_pretrained(context.artifacts[\"pretrained_model_path\"])\n",
    "    self.model = PeftModel.from_pretrained(self.model_pretrained, context.artifacts[\"lora_adatper_path\"])\n",
    "\n",
    "  def predict(self, context, model_input ):\n",
    "    import json\n",
    "    question = model_input.iloc[:,0].to_list()[0] # get the first column\n",
    "\n",
    "    inputs = self.tokenizer(question, return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "    outputs = self.model.generate(input_ids=inputs[\"input_ids\"], max_new_tokens=300)\n",
    "    generated_sql = self.tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0] \n",
    "\n",
    "    result = {'code': generated_sql}\n",
    "    return json.dumps(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da187ccd-aa7b-47e7-a45e-8f7be5b8d22c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sys import version_info\n",
    " \n",
    "PYTHON_VERSION = \"{major}.{minor}.{micro}\".format(major=version_info.major,\n",
    "                                                  minor=version_info.minor,\n",
    "                                                  micro=version_info.micro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8bcabf6-1298-47ff-ab88-e7a6600872df",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import cloudpickle\n",
    "conda_env = {\n",
    "    'channels': ['defaults'],\n",
    "    'dependencies': [\n",
    "      'python={}'.format(PYTHON_VERSION),\n",
    "      'pip',\n",
    "      {\n",
    "        'pip': [\n",
    "          'mlflow',\n",
    "          'transformers==4.27.2',\n",
    "          \"datasets==2.9.0\",\n",
    "          \"accelerate==0.17.1\",\n",
    "          \"evaluate==0.4.0\",\n",
    "          \"bitsandbytes==0.37.1\",\n",
    "          \"peft\",\n",
    "          'pandas',\n",
    "          \"sentencepiece\",\n",
    "          'cloudpickle=={}'.format(cloudpickle.__version__),\n",
    "          'torch'],\n",
    "      },\n",
    "    ],\n",
    "    'name': 'code_env'\n",
    "}\n",
    "\n",
    "mlflow_pyfunc_model_path = \"flant5base_text_to_sql\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cacc0583-3476-4070-bf48-e601d5d54391",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.pyfunc.log_model(artifact_path=mlflow_pyfunc_model_path, python_model=TextToSQL(),artifacts=artifacts, conda_env=conda_env, input_example = input_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4299573d-8baa-4ea9-bc0d-96e16f6fed17",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Once the model is logged, it can be registered to the mlflow model registry.Versioned, tested,transitioned to the deployment stage and deployed following the instructions given here: https://docs.databricks.com/machine-learning/model-serving/create-manage-serving-endpoints.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "99a78340-3099-4309-a22a-5a0bcfe910b4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 753809234887602,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "Step 1_ Fine tuning FLAN-T5-XL for Text to SQL generation with Hugging Face Transformers and Accelerate on Databricks",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
